{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from PIL import Image\n",
    "import tifffile\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "\n",
    "# Use MPS if available?\n",
    "USE_MPS = False # CNN stuff doesn't work (particularly 3D CNN) with MPS. Only CUDA works.\n",
    "\n",
    "# Use Lower Quality features (bigger dataset but quite possibly shit data) for training?\n",
    "USE_LQ_FEATURES = False\n",
    "\n",
    "# Use ensemble method or use padding?\n",
    "USE_ENSEMBLE = False\n",
    "\n",
    "LOAD_WEIGHTS = False # Load pretrained weights if they exist?\n",
    "TRAIN_MODEL = True # Train model? Can train on pre-loaded weights also\n",
    "SAVE_WEIGHTS = True # Save weights after training?\n",
    "\n",
    "ENSEMBLE_WEIGHTS = \"ensemble_model_weights.pth\" # Weights for ensemble learning model\n",
    "PADDING_WEIGHTS = \"padding_model_weights.pth\" # Weights for padding based model\n",
    "\n",
    "RANDOM_SEED = 42069 # Seed to use for all random operations\n",
    "\n",
    "hq_csv_file_path = 'full_Table_HIGH_QUAL.csv'\n",
    "lq_csv_file_path = 'full_Table_STRIPPED_CLEANED.csv'\n",
    "image_folder_path = \"./Images_Of_Networks/tiff/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94227/2855032148.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['cell_group'] = data['cc_pixel_intensity_ratio']\n",
      "/tmp/ipykernel_94227/2855032148.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['image_name'] = data['folder_name_x'] + '_' + data['cc_x'].astype(str)\n",
      "/tmp/ipykernel_94227/2855032148.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['file_name'] = data['image_name'] + '.tif'\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('full_table_all_Cells_Uncleaned.csv')\n",
    "\n",
    "# cleaning the data \n",
    "import io\n",
    "\n",
    "csv_file_path = 'full_table_all_Cells_Uncleaned.csv'\n",
    "buffer = io.StringIO()\n",
    "\n",
    "# Open and process the CSV file, to strip entries so that numbers aren't read as strings by read_csv and column names have no leading/trailing whitespaces\n",
    "with open(csv_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        cleaned_line = ','.join(cell.strip() for cell in line.split(','))\n",
    "        buffer.write(cleaned_line + '\\n')\n",
    "\n",
    "# Move the buffer cursor to the start\n",
    "buffer.seek(0)\n",
    "\n",
    "# Read the cleaned data into pandas\n",
    "data = pd.read_csv(buffer)\n",
    "edge_columns = [col for col in data.columns if col.startswith('edge_') and col.split('_')[-1].isdigit()]\n",
    "# Replace empty entries with 0 in these colu\"mns\n",
    "data[edge_columns] = data[edge_columns].fillna(0)\n",
    "\n",
    "data['cell_group'] = data['cc_pixel_intensity_ratio']\n",
    "data['image_name'] = data['folder_name_x'] + '_' + data['cc_x'].astype(str)\n",
    "data['file_name'] = data['image_name'] + '.tif'\n",
    "\n",
    "\n",
    "# Create the new column based on the condition\n",
    "data['cell_group'] = data['cell_group'].apply(lambda x: 1 if x > 0.1 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the column has Angle or Edge, drop it\n",
    "# rename angle flag to ngle_flag\n",
    "\n",
    "data = data.rename(columns = {'Angle_Flag': 'ngle_Flag'})\n",
    "data = data.rename(columns = {'edges' : 'temp_ed'})\n",
    "\n",
    "data = data.drop(data.columns[data.columns.str.contains('Angle')], axis = 1)\n",
    "data = data.drop(data.columns[data.columns.str.contains('Edge')], axis = 1)\n",
    "data = data.drop(data.columns[data.columns.str.contains('edge')], axis = 1)\n",
    "\n",
    "# so first I wrote a script that takes foldernamex , “_” , the file index to combine it into file_name and appended .tif to it do this\n",
    "\n",
    "data['file_name'] = data['folder_name_x'] + '_' + data['cc_x'].astype(str) + '.tif'\n",
    "# rename 'clustering coefficient' to 'clustering_coefficent'\n",
    "data = data.rename(columns = {'clustering coefficient': 'clustering_coefficient', 'ngle_Flag': 'Angle_Flag', 'temp_ed': 'edges'})\n",
    "\n",
    "# do not remove the 'degree_distribution' column\n",
    "# remove the Nan values from the dataframe\n",
    "data = data.dropna(subset = ['degree_distribution'])\n",
    "# fill Angle_Flag with 0\n",
    "data['Angle_Flag'] = data['Angle_Flag'].fillna(0)\n",
    "\n",
    "clean_df = pd.read_csv(hq_csv_file_path)\n",
    "\n",
    "data = data.fillna(0, )\n",
    "\n",
    "for x in data.columns: \n",
    "    if x == 'degree_distribution': \n",
    "        continue\n",
    "    if x not in clean_df.columns: \n",
    "        data = data.drop(x, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop none in the degree distribution column\n",
    "# fill nan with 0 \n",
    "data \n",
    "import ast\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        print(type(x))\n",
    "        return x\n",
    "    \n",
    "data['degree_distribution'] = data['degree_distribution'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = max(max(row.keys(), default=0) for row in data['degree_distribution'])\n",
    "\n",
    "# Initialize columns with zeros\n",
    "for i in range(1, max_degree + 1):\n",
    "    data[f'Degree_{i}'] = 0\n",
    "\n",
    "# Populate the columns with the counts from the degree distribution\n",
    "for index, row in data.iterrows():\n",
    "    for degree, count in row['degree_distribution'].items():\n",
    "        data.at[index, f'degree_distribution_{degree}'] = count\n",
    "\n",
    "# Drop the original 'degree_distribution' column\n",
    "data.drop('degree_distribution', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree_1\n",
      "Degree_2\n",
      "Degree_3\n",
      "Degree_4\n",
      "Degree_5\n",
      "Degree_6\n",
      "Degree_7\n",
      "Degree_8\n",
      "Degree_9\n",
      "Degree_10\n",
      "Degree_11\n"
     ]
    }
   ],
   "source": [
    "for x in data.columns: \n",
    "    if x not in clean_df.columns: \n",
    "        data = data.drop(x, axis = 1)\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in clean_df.columns: \n",
    "    if y not in data.columns: \n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('All_Groups.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
